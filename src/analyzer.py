#!/usr/bin/env python3
"""AI ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„ê¸° - OpenRouter ì—°ë™ (ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ)"""

import json
import os
from datetime import datetime, timezone, timedelta
from pathlib import Path

import requests
from openai import OpenAI

KST = timezone(timedelta(hours=9))
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
MAX_TITLES = 500

# ì‚¬ìš©í•  ëª¨ë¸
COMPARE_MODELS = [
    "google/gemma-3-27b-it:free",
    "z-ai/glm-4.5-air:free",
    "openai/gpt-oss-120b:free",
]


def load_recent_scrapes(max_entries: int = 1) -> list[dict]:
    """ìµœê·¼ NíšŒ ìŠ¤í¬ë˜í•‘ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    script_dir = Path(__file__).parent.parent
    log_dir = script_dir / "data" / "logs"

    now = datetime.now(KST)
    entries: list[tuple[datetime, dict]] = []

    for i in range(2):
        date = now - timedelta(days=i)
        log_file = log_dir / f"{date.strftime('%Y-%m-%d')}.json"

        if not log_file.exists():
            continue

        with open(log_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        for entry in data:
            try:
                collected_at = datetime.fromisoformat(entry["collected_at"])
            except Exception:
                continue
            entries.append((collected_at, entry))

    if not entries:
        return []

    entries.sort(key=lambda x: x[0])
    if max_entries < 1:
        max_entries = 1
    entries = entries[-max_entries:]

    all_posts: list[dict] = []
    for _, entry in entries:
        for post in entry.get("posts", []):
            post["collected_at"] = entry.get("collected_at")
            all_posts.append(post)

    seen_ids = set()
    unique_posts = []
    for post in all_posts:
        post_id = post.get("id")
        if post_id and post_id in seen_ids:
            continue
        if post_id:
            seen_ids.add(post_id)
        unique_posts.append(post)

    return unique_posts


def analyze_with_ai(posts: list[dict], model: str, client: OpenAI) -> str | None:
    """ì§€ì •ëœ ëª¨ë¸ë¡œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    titles = [f"- {post['title']}" for post in posts[:MAX_TITLES]]
    titles_text = "\n".join(titles)

    prompt = f"""ë‹¤ìŒì€ ë½ë¿Œ ë¦´ë ˆì´ ê²Œì‹œíŒì—ì„œ ìµœê·¼ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì œëª©ë“¤ì…ë‹ˆë‹¤.
ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¸ë Œë“œ ë¶„ì„ê³¼ SNS í™ë³´ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

## ë¶„ì„ ìš”ì²­ì‚¬í•­
1. ì¸ê¸° í‚¤ì›Œë“œ: ìì£¼ ì–¸ê¸‰ë˜ëŠ” ë¸Œëœë“œ/ì„œë¹„ìŠ¤/ì´ë²¤íŠ¸ (ìƒìœ„ 5ê°œ, ê°„ë‹¨íˆ)
2. íŠ¸ë Œë“œ ìš”ì•½: í˜„ì¬ ì–´ë–¤ ì¢…ë¥˜ì˜ ì´ë²¤íŠ¸/í˜œíƒì´ ì£¼ë¡œ ì˜¬ë¼ì˜¤ëŠ”ì§€ (2-3ë¬¸ì¥)

## SNS í™ë³´ ë¬¸êµ¬
"ìŒ€ë¨¹" (ssalmug.com) ê´€ë ¨ X/ìŠ¤ë ˆë“œ ê²Œì‹œë¬¼ì„ 1ê°œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ìŒ€ë¨¹ íŠ¹ì§• (1-2ê°œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ):
- ë ˆí¼ëŸ´ ë§í¬ ë³µë¶™í•˜ë©´ AIê°€ ì•Œì•„ì„œ ë¶„ë¥˜í•´ì¤Œ
- ëˆ„ê°€ ë‚´ ë§í¬ ëˆ„ë¥´ë©´ ë‹µë°©í•˜ê¸° í¸í•¨
- ì˜¤ë˜ëœ ë§í¬ë„ ê³µì •í•˜ê²Œ ë…¸ì¶œë¨

ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  í†¤ì•¤ë§¤ë„ˆ:
- ë¶„ì„ì  ë§íˆ¬ì™€ ì œì•ˆí˜• ë¬¸ì¥
- "~í•´ë³´ì„¸ìš”", "~ìˆì–´ìš”" ê°™ì€ ê´‘ê³  ë§íˆ¬ ì ˆëŒ€ ê¸ˆì§€
- ì´ëª¨ì§€ëŠ” 1-2ê°œë§Œ, ì—†ì–´ë„ ë¨
- í•´ì‹œíƒœê·¸ 2ê°œ ì´í•˜
- 100ì ë‚´ì™¸ë¡œ ì§§ê²Œ
- ë§ˆì¹˜ ë³¸ì¸ì´ ì§ì ‘ ì¨ë³¸ í›„ê¸°ì²˜ëŸ¼

ì¢‹ì€ ì˜ˆì‹œ:
- "ìš”ì¦˜ ì¼€ì´ë±…í¬ ì´ë²¤íŠ¸ ìŒ€ë¨¹ì—ì„œ ë³´ê³  ì‹ ì²­í–ˆëŠ”ë° ë‹µë°©ë„ ë°”ë¡œ ë¨ ã…‹ã…‹"
- "ì¶”ì²œì¸ ë§í¬ ì •ë¦¬í•˜ê¸° ê·€ì°®ì•˜ëŠ”ë° ìŒ€ë¨¹ ì“°ë‹ˆê¹Œ ë³µë¶™ë§Œ í•˜ë©´ ì•Œì•„ì„œ ë¶„ë¥˜í•´ì¤Œ"
- "ì•Œëœ°í° ê°ˆì•„íƒ€ë ¤ê³  ìŒ€ë¨¹ ë“¤ì–´ê°”ë‹¤ê°€ ì¼€ë±… ëˆë‚˜ë¬´ë„ ë°œê²¬ ğŸ€"

ë‚˜ìœ ì˜ˆì‹œ (ì´ë ‡ê²Œ ì“°ì§€ ë§ ê²ƒ):
- "ìŒ€ë¨¹ì—ì„œ ë‹¤ì–‘í•œ í˜œíƒì„ ë§Œë‚˜ë³´ì„¸ìš”!"
- "ì¶”ì²œì¸ í”„ë¡œê·¸ë¨ê³¼ í•¨ê»˜ ì¦ê±°ìš´ ê²½í—˜ì„ í•´ë³´ì„¸ìš”~"

## ê²Œì‹œë¬¼ ì œëª© ({len(posts)}ê°œ)
{titles_text}

í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model=model,
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}],
        )

        content = response.choices[0].message.content
        if content and content.strip():
            return content
        return None

    except Exception as e:
        print(f"  ëª¨ë¸ {model} ì‹¤íŒ¨: {e}")
        return None


def analyze_with_multiple_models(posts: list[dict]) -> list[dict]:
    """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë¶„ì„í•˜ì—¬ ëª¨ë“  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    api_key = os.environ.get("OPENROUTER_API_KEY")
    if not api_key:
        return [{"model": "error", "analysis": "Error: OPENROUTER_API_KEY not set"}]

    client = OpenAI(
        base_url=OPENROUTER_BASE_URL,
        api_key=api_key,
    )

    results = []

    for model in COMPARE_MODELS:
        print(f"ëª¨ë¸ ì‹œë„: {model}")
        result = analyze_with_ai(posts, model, client)
        if result:
            results.append({
                "model": model,
                "analysis": result,
            })
            print(f"  âœ“ ì„±ê³µ")
        else:
            print(f"  âœ— ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ì‘ë‹µ")

    return results


def build_error_result(message: str) -> list[dict]:
    """ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë³´ë‚´ê¸° ìœ„í•œ ë‹¨ì¼ ê²°ê³¼ í˜•ì‹."""
    return [{"model": "error", "analysis": message}]


def save_analysis(results: list[dict], post_count: int) -> str:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    now = datetime.now(KST)
    date_str = now.strftime("%Y-%m-%d")

    script_dir = Path(__file__).parent.parent
    analysis_dir = script_dir / "data" / "analysis"
    analysis_dir.mkdir(parents=True, exist_ok=True)

    analysis_file = analysis_dir / f"{date_str}.json"

    if analysis_file.exists():
        with open(analysis_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = []

    entry = {
        "analyzed_at": now.isoformat(),
        "post_count": post_count,
        "results": results,  # ì—¬ëŸ¬ ëª¨ë¸ ê²°ê³¼
    }
    data.append(entry)

    with open(analysis_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return str(analysis_file)


def main():
    print(f"[{datetime.now(KST).isoformat()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")

    env_value = os.environ.get("ANALYSIS_RECENT_SCRAPES", "").strip()
    try:
        recent_scrapes = int(env_value) if env_value else 6
    except ValueError:
        recent_scrapes = 6
    posts = load_recent_scrapes(recent_scrapes)
    print(f"ë¶„ì„ ëŒ€ìƒ ê²Œì‹œë¬¼: {len(posts)}ê°œ (ìµœê·¼ {recent_scrapes}íšŒ ìŠ¤í¬ë˜í•‘)")

    if len(posts) < 5:
        print("ë¶„ì„í•˜ê¸°ì— ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 5ê°œ í•„ìš”)")
        results = build_error_result(
            f"ë¶„ì„ ë¶ˆê°€: ë°ì´í„° ë¶€ì¡± ({len(posts)}ê°œ, ìµœì†Œ 5ê°œ í•„ìš”)"
        )
        analysis_file = save_analysis(results, len(posts))
        print(f"\nì €ì¥ ì™„ë£Œ: {analysis_file}")
        return

    results = analyze_with_multiple_models(posts)
    if not results:
        results = build_error_result("ë¶„ì„ ì‹¤íŒ¨: ëª¨ë“  ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ì‘ë‹µ")

    print(f"\n=== ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ëª¨ë¸ ì„±ê³µ ===")
    for r in results:
        print(f"\n--- {r['model']} ---")
        print(r['analysis'][:200] + "..." if len(r['analysis']) > 200 else r['analysis'])

    analysis_file = save_analysis(results, len(posts))
    print(f"\nì €ì¥ ì™„ë£Œ: {analysis_file}")


if __name__ == "__main__":
    main()
